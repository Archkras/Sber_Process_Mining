# Numpy Python module is used in this file.
#   Licence: BSD-3-Clause License
#   Link: https://github.com/numpy/numpy

# Pandas Python module is used in this file.
#   Licence: BSD-3-Clause License
#   Link: https://github.com/pandas-dev/pandas

# Matplotlib Python module is used in this file.
#   Licence: BSD compatible
#   Link: https://github.com/matplotlib/matplotlib

from IPython.display import clear_output
from collections import defaultdict
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from ._exploitation_env import ExploitationEnvironment
from typing import Union


class ExploitationAgent:
    """
    Implementation of an agent acting in accordance with the exploitation strategy.

    Parameters
    ----------
    env: sberpm.ml.reinforcement_learning._exploitation_env.ExploitationEnvironment
        Environment object.

    discount: float, default=0.9
        A parameter that determines the agent's attention on immediate and distant rewards.

    alpha: float, default=0.9
        Learning rate for updating the agent.
    """

    def __init__(self, env: ExploitationEnvironment,
                 discount: float = .9,
                 alpha: float = .1):

        self.qvalues = defaultdict(lambda: defaultdict(lambda: 0))
        self.transition_counter = defaultdict(lambda: defaultdict(lambda: []))
        self.discount = discount
        self.alpha = alpha
        self.env = env

    def fit(self, n_iter: int = 1000, verbose: bool = True) -> pd.DataFrame:
        """
        Trains the agent for a given number of iterations and forms a
        dataframe containing the result of the agent's work.

        Parameters
        ----------
        n_iter: int, default=1000
            Number of iterations.

        verbose: bool, default=True
            If True, plots the reward graph representing the learning process.

        Returns
        -------
        result: pd.DataFrame
        """
        plot_percent = int(n_iter * 0.05)
        trace_rewards = []
        mean_reward = []
        iterations = []
        for i in range(n_iter):
            self.env.new()

            total_reward = self.play_session()
            if verbose:
                trace_rewards.append(total_reward)
                if i % plot_percent == 0:
                    clear_output(True)
                    mean_reward.append(np.mean(trace_rewards[-plot_percent:]))
                    iterations.append(i)
                    plt.title('mean reward = {:.1f}'.format(mean_reward[-1]))
                    plt.plot(iterations, mean_reward)
                    plt.xlabel("Iterations")
                    plt.ylabel("Mean rewards")
                    plt.show()

        return self._desc_qvalues()

    def play_session(self) -> float:
        """
        Performs agent training prior to staying in final activity.

        Returns
        -------
        total_reward: float
        """
        s = self.env.current_state
        if s != self.env.initial_state:
            s = self.env.new()

        total_reward = 0
        while not self.env.is_done:
            next_s, r, _ = self.env.step()
            self._update(s, next_s, r)
            s = next_s
            total_reward += r

        return total_reward

    def _get_action_value(self, state: str) -> float:
        """
        Calculates the best action value for a given state.

        Parameters
        ----------
        state: str

        Returns
        -------
        value: float
        """
        possible_actions = self.transition_counter[state].keys()

        if len(possible_actions) == 0:
            return 0.0

        action_values = list(map(lambda action: self.qvalues[state][action], possible_actions))
        value = max(action_values)

        return value

    def get_best_action(self, state: str) -> Union[str, None]:
        """
        Returns the best action for the given state based on their Q-values.

        Parameters
        ----------
        state: int

        Returns
        -------
        best_action: str
        """
        possible_actions = self.transition_counter[state].keys()
        possible_actions = list(filter(lambda a: len(self.transition_counter[state][a]) > 0, possible_actions))

        if len(possible_actions) == 0:
            return None

        qvalues = list(map(lambda action: self.qvalues[state][action], possible_actions))
        best_action = possible_actions[np.argmax(qvalues)]
        return best_action

    def _update(self, state: str, action: str, reward: float) -> None:
        """
        Updates agent's Q-values according to update rule.

        Parameters
        ----------
        state: str
        action: str
        reward: float
        """
        gamma = self.discount
        learning_rate = self.alpha

        next_state = action
        new_qvalue = (1 - learning_rate) * self.qvalues[state][action] + learning_rate * (
                reward + gamma * self._get_action_value(next_state))

        self.qvalues[state][action] = new_qvalue
        self.transition_counter[state][action].append(1)

    def calculate_cum_reward(self, rewards) -> float:
        """
        Calculates cumulative rewards.

        Parameters
        ----------
        rewards: dict

        Returns
        -------
        result: float
        """
        return sum(map(lambda x: pow(self.discount, x[0]) * x[1], enumerate(rewards)))

    def _desc_qvalues(self) -> pd.DataFrame:
        """
        Forms dataframe containing state-action Q-values.

        Returns
        -------
        result: pd.DataFrame
        """
        desc_df = pd.DataFrame(index=self.transition_counter.keys())

        for state in self.transition_counter.keys():
            for action in self.transition_counter[state].keys():
                desc_df.loc[state, action] = int(self.qvalues[state][action])

        return desc_df.fillna(0)

    def reset(self) -> None:
        """
        Reset Q-values.
        """
        self.qvalues = defaultdict(lambda: defaultdict(lambda: 0.))
