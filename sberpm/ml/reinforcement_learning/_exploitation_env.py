# Numpy Python module is used in this file.
#   Licence: BSD-3-Clause License
#   Link: https://github.com/numpy/numpy

# Pandas Python module is used in this file.
#   Licence: BSD-3-Clause License
#   Link: https://github.com/pandas-dev/pandas


from ...metrics import TransitionMetric
from collections import Counter
import pandas as pd
import numpy as np

from ..._holder import DataHolder
from typing import Iterator, Tuple, Dict, Optional, List, Iterable


class TraceGenerator:
    """
    A class that generates the traces for training an agent.
    """

    def __init__(self, data_holder: DataHolder,
                 initial_state: str,
                 terminal_state: str,
                 shuffle: bool = True):
        self.act_col = data_holder.activity_column
        self.tw_col = data_holder.duration_column
        self.grouped_data = data_holder.get_grouped_data(self.act_col, self.tw_col)
        if shuffle:
            self.grouped_data = self.grouped_data.sample(frac=1)
        self.trace_gen = self.trace_generator()
        self.initial_state = initial_state
        self.terminal_state = terminal_state

    def trace_generator(self) -> Iterator[pd.Series]:
        """
        Returns an iterator that yields traces:
        pandas Series with two columns: tuple of activities and tuple of time durations.
        """
        for _, row in self.grouped_data.copy().iterrows():
            row[self.act_col] = (self.initial_state,) + row[self.act_col] + (self.terminal_state,)
            row[self.tw_col] = (np.nan,) + row[self.tw_col] + (np.nan,)
            yield row

    def generate(self) -> pd.Series:
        """
        Returns an event trace:
        pandas Series with two columns: tuple of activities and tuple of time durations.
        """
        try:
            return next(self.trace_gen)
        except StopIteration:
            self.trace_gen = self.trace_generator()
            return next(self.trace_gen)


class ExploitationEnvironment:
    """
    Class that constructs the environment for the exploitation strategy.

    Parameters
    ----------
    data_holder: sberpm.DataHolder
        Object that contains the event log and the names of its necessary columns (id, activities, timestamps, etc.).

    shuffle: bool, default=True
        If True, the order of the traces will be changed randomly.
    """

    def __init__(self, data_holder: DataHolder, shuffle: bool = True):
        data_holder.check_or_calc_duration()
        self.shuffle = shuffle
        self.initial_state = 'START_EVENT'
        self.terminal_state = 'END_EVENT'
        self.actions = np.unique(data_holder.data[data_holder.activity_column])
        self.states = self.actions
        self.actions = np.append(self.actions, self.terminal_state)
        self.states = np.append(self.states, self.initial_state)
        self.activity_column = data_holder.activity_column
        self.transition_times = self.calculate_mean_trans_durations(data_holder)
        self.trace_gen = TraceGenerator(data_holder, self.initial_state, self.terminal_state)
        self.trace = self.trace_gen.generate()
        self.current_state = self.initial_state
        self.pos_in_trace = 0
        self.is_done = False
        self.reward_design = None
        self.key_states = None

    def step(self) -> Tuple[str, float, bool]:
        """
        Goes to the next step in the trace.

        Returns
        -------
        self.current_state: str

        reward: float

        self.is_done: bool
        """
        if self.is_done:
            return self.current_state, 0, self.is_done

        self.pos_in_trace += 1

        _, states, times = self.trace
        self.current_state = states[self.pos_in_trace]
        reward = self.calculate_reward()

        if self.current_state == self.terminal_state:
            self.is_done = True

        return self.current_state, reward, self.is_done

    def define_rewards(self, reward_design: Dict[str, float],
                       key_states: Optional[List[str]] = None) -> None:
        """
        Defines reward design and states with the increased reward.

        Parameters
        ----------
        reward_design: dict

        key_states: list, default=None
            Transition to these states will be rewarded additionally.
        """
        self.reward_design = reward_design
        self.key_states = key_states

    def new(self) -> str:
        """
        Goes to the next trace obtained by the class TraceGenerator.

        Returns
        -------
        self.current_state: str
        """
        self.trace = self.trace_gen.generate()
        self.current_state = self.initial_state
        self.pos_in_trace = 0
        self.is_done = False

        return self.current_state

    @staticmethod
    def calculate_mean_trans_durations(data_holder: DataHolder) -> Dict[Tuple[str, str], float]:
        """
        Calculates mean transition durations.

        Parameters
        ----------
        data_holder: sberpm.DataHolder

        Returns
        -------
        result: dict
        """
        tm = TransitionMetric(data_holder, time_unit='m')
        return {row[tm._group_column]: row['mean_duration'] for _, row in
                tm.mean_duration().reset_index(drop=False).iterrows()}

    def calculate_reward(self) -> float:
        """
        Calculates rewards for current activity in the trace.

        Returns
        -------
        reward: float
        """
        _, states, times = self.trace

        if self.pos_in_trace == 0:
            state = self.initial_state
        else:
            state = states[self.pos_in_trace - 1]
        action = states[self.pos_in_trace]
        reward = 0
        # reward by state
        if self.key_states:
            if state in self.key_states:
                reward = self.reward_design['increased_reward']
        else:
            reward = self.reward_design['default_reward']

        # reward by time
        mean_transition_time = self.transition_times.get((state, action))
        if not pd.isna(mean_transition_time) and not pd.isna(times[self.pos_in_trace]):
            duration_reward = np.clip(mean_transition_time / (times[self.pos_in_trace] + .0001), 0,
                                      self.reward_design['duration_reward'])
            reward += duration_reward

            # reward by cycle
        if states[self.pos_in_trace] in states[:self.pos_in_trace]:
            reward -= self.reward_design['cycle_penalty']

        # reward finish
        if action == self.terminal_state:
            num_cycles = sum(np.array(list(Counter(self.trace[self.activity_column]).values())) - 1)
            reward += self.reward_design['finish_reward']
            if num_cycles == 0:
                reward += self.reward_design['final_cycle_reward']

        return reward

    def get_legal_actions(self) -> Iterable[str]:
        """
        Returns possible actions.

        Returns
        -------
        actions: list of str
        """
        return self.actions
